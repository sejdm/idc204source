

### Are there languages that a Turing machine cannot recognize?

Remember that the set of languages is uncountable because it is a power set of $\Sigma^*$. A Turing machine can be specified by a (finite) string, so the set of Turing machines is countable. If I correspond every Turing machine to the language it recognizes, there are bound to be many (in fact, uncountable) languages that are left out.

The same reasoning as above works for finite state automata as well as push-down automata. Of course, any subset of $\Sigma^*$ is technically a language but we want to find examples of saner languages that cannot be recognized. For finite state automata and push-down automata, we found concrete examples and used the pumping lemma to prove it. For Turing machines, the situation is trickier but later in the course we will see why it is impossible to build a Turing machine that will identify whether or a not a given Turing machine eventually will halt if a given string is provided as its input. Therefore, if I consider the language consisting of those pairs of the form $(T, s)$ such that $T$ will eventually halt when provided with $s$ as an input, then this language cannot be recognized by a Turing machine.

### Why finite states but an infinite tape

The transition function is at the heart of a finite state automaton, push-down automaton, or Turing machine. In each case, the finite state and alphabet together ensure that the domain is finite, allowing the function to be specified as a simple table. A particular Turing machine may need larger memory for larger inputs and is, therefore, provided with an infinite tape but if the states are infinite, you cannot even specify the complete design of the machine. 

In practice, we do not have infinite memory for any computation, but usually we have more than enough for the task at hand. Even if we temporarily run out of memory, it is simpler to add more memory. For instance, if we had a finite tape we may be forced to compromise and accept inputs from only a subset of the language. But if we want to accept bigger inputs, we just need to add more cells and lengthen the tape. On the other hand, consider what happens if we tried a similar compromise with a finite state automaton: if we try to recognize a non-regular language by bothering about only a finite subset of the language. Since all finite languages are regular, we can build a finite state automaton to recognize just that subset. However, every time we want to enhance that automaton to recognize an even bigger subset, the automaton will need a complete redesign! This is not at all as simple as increasing the length of the tape.

In fact, although technically we can design a finite state automaton to recognize a finite subset of a non-regular language, it will most likely require a huge number of states and will be little better than simply listing out all the elements of the language explicitly. The transition function will be tedious. But if we can build a Turing machine to recognize the entire language, although a finite tape may limit its use to a subset of the language, since larger inputs may at most require more tape with no change to the transition function, the transition function is likely to be significantly more sophisticated than merely listing out all the elements.

Furthermore, for a given computational task, if we assume unlimited memory, we can first establish whether or not it is possible to design a Turing machine at all for that task. Once that is done, we can study the maximum memory requirement (and even the time it takes) and in fact try to understand how that changes with the size of the input. We can know, for instance, whether a particular computation needs only a fixed amount of memory, even for large inputs, or if the size required grows exponentially if we increase the size of the input. This is precisely the concern of complexity theory, a subject that is tackled in your textbook although we may not have the time to touch upon it in this course. So we can say that by allowing an infinite tape, we can at least first tackle the problem of whether the computation is possible in theory, assuming as much memory available as needed, and study the impacts of limited memory separately (there is standard way to specify that, eg. the big O notation).

If the states are infinite, we cannot represent the transition function explicitly. One may ask why we insist that the transition function be defined explicitly. After all, we are used to many functions with infinite domains that we represent by using formulae. But we are trying to reduce computations to the basics. If you use a formula to represent a transition function with an infinite domain, you will have to specify how to compute that transition function so it may be a minor simplification at best but is not comparable to the reduction to basics achieved by Turing machines. So many computations, from arithmetic to sorting lists, in fact, anything that you can program, can be modelled using Turing machines. Yet, the differences between Turing machine is reduced to merely differences in the precise entries of the transition table. The form of the table, the way it is used by the machine along with the tape etc. are all the same for all Turing machine no matter what they model!

Finally, whenever you wonder why an alernative model is not used instead of the conventional Turing machine, the answer is either that your model is not as powerful or it is exactly as powerful, in which case choosing between your model and the conventional one is just a matter of taste. For instance, if you allowed transition functions with infinite domains, you would have to restrict attention to those functions which can be defined by at least finite amount of information so that you can at least specify the machine. So you might consider a Turing like machine which works just like an ordinary one except that its transition function is allowed to compute outputs using a conventional Turing machine.  However, since the workings of this machine can be simulated by a conventional Turing machine, this too will not be more powerful, despite enhancing its transition function. In fact, Turing, Church, and Goedel made completely independent attempts to define computable functions, but the remarkable fact is that each method, despite looking completely independent of each other, characterizes exactly the same class of functions as computable. The same holds true for subsequent attempts at characterizing computable functions (Read about the [Church-Turing thesis](https://en.wikipedia.org/wiki/Churchâ€“Turing_thesis), if you are interested). Therefore, although each method may individually feel a bit artificial, the fact that they characterize the same functions as computable makes one feel that they capture the concept of computation correctly. Yet another reason why the conventional definition of a Turing machine, with its finite set of states and infinite tape, ough to be the "right" definition even if there is more than one "right" definition.

